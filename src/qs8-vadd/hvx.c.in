// Copyright 2024 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.

$assert DATATYPE in ["QS8", "QU8"]
$assert BATCH_TILE % 8 == 0
$assert BATCH_TILE >= 8
#include <assert.h>

#include <hvx_hexagon_protos.h>
#include <hexagon_protos.h>
#include <hexagon_types.h>

#include <xnnpack/vbinary.h>
#include <xnnpack/intrinsics-polyfill.h>

$XINT8_T = {"QS8": "int8_t", "QU8": "uint8_t"}[DATATYPE]
void xnn_${DATATYPE.lower()}_vadd_minmax_ukernel__hvx_u${BATCH_TILE}(
    size_t batch,
    const ${XINT8_T}* input_a,
    const ${XINT8_T}* input_b,
    ${XINT8_T}* output,
    const union xnn_${DATATYPE.lower()}_add_minmax_params params[restrict XNN_MIN_ELEMENTS(1)]) XNN_OOB_READS
{
  assert(batch != 0);
  assert(batch % sizeof(${XINT8_T}) == 0);
  assert(input_a != NULL);
  assert(input_b != NULL);
  assert(output != NULL);

  const HVX_Vector vbias = Q6_V_vsplat_R(*((int32_t *) &params->hvx.bias));
  const HVX_Vector va_multiplier = Q6_V_vsplat_R(*((int32_t *) &params->hvx.a_multiplier));
  const HVX_Vector vb_multiplier = Q6_V_vsplat_R(*((int32_t *) &params->hvx.b_multiplier));
  const int32_t first_shift = params->hvx.first_shift;
  const int32_t rest_shift = params->hvx.rest_shift;
  const HVX_Vector voutput_zero_point = Q6_Vh_vsplat_R(*((int16_t *) &params->hvx.output_zero_point));
  const HVX_Vector voutput_min = Q6_Vb_vsplat_R(*((int8_t *) &params->hvx.output_min));
  const HVX_Vector voutput_max = Q6_Vb_vsplat_R(*((int8_t *) &params->hvx.output_max));
  int8_t* ptr_o = output;

  for (; batch >= ${BATCH_TILE} * sizeof(${XINT8_T}); batch -= ${BATCH_TILE} * sizeof(${XINT8_T})) {
    HVX_Vector va0 = *((HVX_UVector*)input_a);
    HVX_Vector vb0 = *((HVX_UVector*)input_b);
    $for N in range(128, BATCH_TILE, 128):
      HVX_Vector va${int(N/32)} = *((HVX_UVector*)(input_a + ${N}));
      HVX_Vector vb${int(N/32)} = *((HVX_UVector*)(input_b + ${N}));
    input_a += ${BATCH_TILE};
    input_b += ${BATCH_TILE};

    // widen: 8-bit to 16-bit
    $for N in range(0, BATCH_TILE, 128):
      HVX_VectorPair va${int(N/32)}_i16 = Q6_Wh_vsxt_Vb(va${int(N/32)});
      HVX_Vector va${int(N/32)}_i16_even = Q6_V_lo_W(va${int(N/32)}_i16);
      HVX_Vector va${int(N/32)}_i16_odd = Q6_V_hi_W(va${int(N/32)}_i16);

      HVX_VectorPair vb${int(N/32)}_i16 = Q6_Wh_vsxt_Vb(vb${int(N/32)});
      HVX_Vector vb${int(N/32)}_i16_even = Q6_V_lo_W(vb${int(N/32)}_i16);
      HVX_Vector vb${int(N/32)}_i16_odd = Q6_V_hi_W(vb${int(N/32)}_i16);

    // vacc = va * va_multiplier + vb * vb_multiplier with widening to 32-bit
    $for N in range(0, BATCH_TILE, 128):
      HVX_VectorPair va${int(N/32)}_mul_even = Q6_Vw_vmpyi_VwVh(va_multiplier, va${int(N/32)}_i16_even);
      HVX_VectorPair va${int(N/32)}_mul_odd = Q6_Vw_vmpyi_VwVh(va_multiplier, va${int(N/32)}_i16_odd);
      HVX_VectorPair vb${int(N/32)}_mul_even = Q6_Vw_vmpyi_VwVh(vb_multiplier, vb${int(N/32)}_i16_even);
      HVX_VectorPair vb${int(N/32)}_mul_odd = Q6_Vw_vmpyi_VwVh(vb_multiplier, vb${int(N/32)}_i16_odd);

      HVX_Vector vacc${int(N/32)}_even = Q6_Vw_vadd_VwVw(Q6_V_lo_W(va${int(N/32)}_mul_even), Q6_V_lo_W(vb${int(N/32)}_mul_even));
      HVX_Vector vacc${int(N/32)}_odd = Q6_Vw_vadd_VwVw(Q6_V_lo_W(va${int(N/32)}_mul_odd), Q6_V_lo_W(vb${int(N/32)}_mul_odd));
      $if BATCH_TILE > N + 64:
        HVX_Vector vacc${int((N+32)/32)}_even = Q6_Vw_vadd_VwVw(Q6_V_hi_W(va${int(N/32)}_mul_even), Q6_V_hi_W(vb${int(N/32)}_mul_even));
        HVX_Vector vacc${int((N+32)/32)}_odd = Q6_Vw_vadd_VwVw(Q6_V_hi_W(va${int(N/32)}_mul_odd), Q6_V_hi_W(vb${int(N/32)}_mul_odd));

    // vacc = vbias + vacc
    $for N in range(0, BATCH_TILE, 128):
      vacc${int(N/32)}_even = Q6_Vw_vadd_VwVw(vbias, vacc${int(N/32)}_even);
      vacc${int(N/32)}_odd = Q6_Vw_vadd_VwVw(vbias, vacc${int(N/32)}_odd);
      $if BATCH_TILE > N + 64:
        vacc${int((N+32)/32)}_even = Q6_Vw_vadd_VwVw(vbias, vacc${int((N+32)/32)}_even);
        vacc${int((N+32)/32)}_odd = Q6_Vw_vadd_VwVw(vbias, vacc${int((N+32)/32)}_odd);

    // vacc = shift and narrow to 16-bit. Add voutput_zero_point
    $for N in range(0, BATCH_TILE, 128):
      HVX_Vector vacc${int(N/32)} = Q6_Vh_vasr_VwVwR_sat(vacc${int(N/32)}_odd, vacc${int(N/32)}_even, first_shift);
      vacc${int(N/32)} = Q6_Vh_vadd_VhVh(voutput_zero_point, Q6_Vh_vasr_VhR(vacc${int(N/32)}, rest_shift));
      $if BATCH_TILE > N + 64:
        HVX_Vector vacc${int((N+32)/32)} = Q6_Vh_vasr_VwVwR_sat(vacc${int((N+32)/32)}_odd, vacc${int((N+32)/32)}_even, first_shift);
        vacc${int((N+32)/32)} = Q6_Vh_vadd_VhVh(voutput_zero_point, Q6_Vh_vasr_VhR(vacc${int((N+32)/32)}, rest_shift));

    // narrow: 16-bit to 8-bit
    $for N in range(0, BATCH_TILE, 128):
      $if BATCH_TILE > N + 64:
        HVX_Vector vout${int(N/32)} = Q6_Vb_vpack_VhVh_sat(vacc${int((N+32)/32)}, vacc${int(N/32)});
      $else:
        HVX_Vector vout${int(N/32)} = Q6_Vb_vpack_VhVh_sat(vacc${int(N/32)}, vacc${int(N/32)});

    // minmax
    $for N in range(0, BATCH_TILE, 128):
      vout${int(N/32)} = Q6_Vb_vmax_VbVb(voutput_min, vout${int(N/32)});
      vout${int(N/32)} = Q6_Vb_vmin_VbVb(voutput_max, vout${int(N/32)});

    // store output
    $for N in range(0, BATCH_TILE, 128):
      Q6_V_vstu_variable(ptr_o, ${BATCH_TILE}, vout${int(N/32)});
      ptr_o += ${BATCH_TILE};
  }
  if XNN_UNLIKELY(batch != 0){
    do {
      HVX_Vector va = *((HVX_UVector*)input_a);
      HVX_Vector vb = *((HVX_UVector*)input_b);
      if XNN_LIKELY(batch > (32 * sizeof(int8_t))) {
        input_a += 32;
        input_b += 32;
      }

      HVX_VectorPair va_i16 = Q6_Wh_vsxt_Vb(va);
      HVX_Vector va_i16_even = Q6_V_lo_W(va_i16);
      HVX_Vector va_i16_odd = Q6_V_hi_W(va_i16);

      HVX_VectorPair vb_i16 = Q6_Wh_vsxt_Vb(vb);
      HVX_Vector vb_i16_even = Q6_V_lo_W(vb_i16);
      HVX_Vector vb_i16_odd = Q6_V_hi_W(vb_i16);

      HVX_VectorPair va_mul_even = Q6_Vw_vmpyi_VwVh(va_multiplier, va_i16_even);
      HVX_VectorPair va_mul_odd = Q6_Vw_vmpyi_VwVh(va_multiplier, va_i16_odd);
      HVX_VectorPair vb_mul_even = Q6_Vw_vmpyi_VwVh(vb_multiplier, vb_i16_even);
      HVX_VectorPair vb_mul_odd = Q6_Vw_vmpyi_VwVh(vb_multiplier, vb_i16_odd);

      HVX_Vector vacc_even = Q6_Vw_vadd_VwVw(Q6_V_lo_W(va_mul_even), Q6_V_lo_W(vb_mul_even));
      HVX_Vector vacc_odd = Q6_Vw_vadd_VwVw(Q6_V_lo_W(va_mul_odd), Q6_V_lo_W(vb_mul_odd));

      vacc_even = Q6_Vw_vadd_VwVw(vbias, vacc_even);
      vacc_odd = Q6_Vw_vadd_VwVw(vbias, vacc_odd);

      HVX_Vector vacc = Q6_Vh_vasr_VwVwR_sat(vacc_odd, vacc_even, first_shift);
      vacc = Q6_Vh_vadd_VhVh(voutput_zero_point, Q6_Vh_vasr_VhR(vacc, rest_shift));

      HVX_Vector vout = Q6_Vb_vpack_VhVh_sat(vacc, vacc);
      vout = Q6_Vb_vmax_VbVb(voutput_min, vout);
      vout = Q6_Vb_vmin_VbVb(voutput_max, vout);

      if XNN_LIKELY(batch > (32 * sizeof(int8_t))) {
        Q6_V_vstu_variable(ptr_o, 32, vout);
        ptr_o += 32;
        batch -=32;
      }
      else{
        Q6_V_vstu_variable(ptr_o, batch, vout);
        batch = 0;
      }
    } while (batch != 0);
  }
}
